{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 元データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/user/documents/quora/train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/user/documents/quora/test.csv\")\n",
    "\n",
    "train_data.fillna(0)\n",
    "test_data.fillna(0)\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questionの文の長さを比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_len = list()\n",
    "test_word_len = list()\n",
    "\n",
    "for i in range(len(train_data['question1'])):\n",
    "    q1_len = len(str(train_data['question1'][i])) if train_data['question1'][i] else 0\n",
    "    q2_len = len(str(train_data['question2'][i])) if train_data['question2'][i] else 0\n",
    "    dis = abs(q1_len - q2_len)\n",
    "    train_word_len.append(dis)\n",
    "\n",
    "for i in range(len(test_data['question1'])):\n",
    "    q1_len = len(str(test_data['question1'][i])) if test_data['question1'][i] else 0\n",
    "    q2_len = len(str(test_data['question2'][i])) if test_data['question2'][i] else 0\n",
    "    dis = abs(q1_len - q2_len)\n",
    "    test_word_len.append(dis)\n",
    "    \n",
    "train_df['word_len'] = train_word_len\n",
    "test_df['word_len'] = test_word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章の単語の一致率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def read_txt(file):\n",
    "    tmp = list()\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            line.replace('\\n', \"\")\n",
    "            tmp.append(line)\n",
    "    return tmp\n",
    "\n",
    "def match_ratio(t1, t2):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    q1 = dict()\n",
    "    q2 = dict()\n",
    "    for word in str(t1.lower().split()):\n",
    "        if word not in stops:\n",
    "            q1[word] = 1\n",
    "    for word in str(t2.lower().split()):\n",
    "        if word not in stops:\n",
    "            q2[word] = 1\n",
    "    if len(q1) == 0 or len(q2) == 0:\n",
    "        return 0\n",
    "    match_words = [w for w in q1.keys() if w in q2]\n",
    "    return (len(match_words)*2 / (len(q1) + len(q2)))\n",
    "\n",
    "train_text = read_txt(\"stem_train.txt\")\n",
    "test_text = read_txt(\"stem_test.txt\")\n",
    "train_match = list()\n",
    "test_match = list()\n",
    "\n",
    "for train in zip(*[iter(train_text)] * 2):\n",
    "    train_match.append(match_ratio(train[0], train[1]))\n",
    "train_df[\"word_match\"] = train_match\n",
    "\n",
    "for test in zip(*[iter(test_text)] * 2):\n",
    "    test_match.append(match_ratio(test[0], test[1]))\n",
    "test_df[\"word_match\"] = test_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章のtfidf値のcos類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def pickle_load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    dot = 0\n",
    "    for value in v1.data:\n",
    "        a += value * value\n",
    "        a = np.sqrt(a)\n",
    "\n",
    "    for value in v2.data:\n",
    "        b += value * value\n",
    "    b = np.sqrt(b)\n",
    "\n",
    "    for word_a, value_a in zip(v1.indices, v1.data):\n",
    "        for word_b, value_b in zip(v2.indices, v2.data):\n",
    "            if word_a == word_b:\n",
    "                dot += value_a * value_b\n",
    "    return (dot / (a * b)) if (a * b) else 0\n",
    "\n",
    "result = list()\n",
    "train_r = list()\n",
    "\n",
    "x = pickle_load(\"train_tfidf.pickle\")\n",
    "y = pickle_load(\"test_tfidf.pickle\")\n",
    "for vec in zip(*[iter(x)] * 2):\n",
    "    train_r.append(cos_sim(vec[0], vec[1]))\n",
    "train_df[\"tfidf_sim\"] = train_r\n",
    "for vec in zip(*[iter(y)] * 2):\n",
    "    result.append(cos_sim(vec[0], vec[1]))\n",
    "test_df[\"tfidf_sim\"] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴語のtfidf値比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def pickle_load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def dis_cal(t1, t2):\n",
    "    return (1 - abs(t1-t2))**2\n",
    "\n",
    "x = pickle_load(\"train_tfidf.pickle\")\n",
    "y = pickle_load(\"test_tfidf.pickle\")\n",
    "\n",
    "train_r = list()\n",
    "test_r = list()\n",
    "\n",
    "for vec in zip(*[iter(x)] * 2):\n",
    "    if vec[0].getnnz() == 0 or vec[1].getnnz() == 0:\n",
    "        train_r.append(0)\n",
    "    else:\n",
    "        train_r.append(dis_cal(vec[0].data.max(), vec[1].data.max()))\n",
    "train_df[\"word_dis\"] = train_r\n",
    "\n",
    "for vec in zip(*[iter(y)] * 2):\n",
    "    if vec[0].getnnz() == 0 or vec[1].getnnz() == 0:\n",
    "        test_r.append(0)\n",
    "    else:\n",
    "        test_r.append((dis_cal(vec[0].data.max(), vec[1].data.max())))\n",
    "test_df[\"word_dis\"] = test_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前置詞のストップワードを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = {\"above\", \"after\", \"against\", \"among\", \"anti\", \"at\", \"before\", \"behind\", \"below\",\n",
    "        \"beside\", \"besides\", \"between\", \"beyond\", \"but\", \"by\", \"considering\",\"despite\", \"during\",\n",
    "        \"for\", \"from\", \"in\", \"inside\", \"into\", \"less\", \"near\", \"of\", \"on\", \"onto\", \"oppesite\",\n",
    "        \"outside\", \"over\", \"past\", \"regarding\", \"since\", \"throughout\", \"till\", \"under\", \"underneath\", \"unless\",\n",
    "        \"unlike\", \"untill\", \"via\", \"with\", \"within\", \"without\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章の前置詞の数を比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pre(t1,t2):\n",
    "    q1 = dict()\n",
    "    q2 = dict()\n",
    "    \n",
    "    for word in t1.lower().split():\n",
    "        if word in stops:\n",
    "            q1[word] = 1\n",
    "    \n",
    "    for word in t2.lower().split():\n",
    "        if word in stops:\n",
    "            q2[word] = 1\n",
    "    \n",
    "    return abs(len(q1) - len(q2))\n",
    "\n",
    "train_pre = list()\n",
    "test_pre = list()\n",
    "\n",
    "for i in range(0, len(train_data)-1):\n",
    "    tmp = count_pre(str(train_data['question1'][i]), str(train_data['question2'][i]))\n",
    "    train_pre.append(tmp)\n",
    "tmp = count_pre(str(train_data['question1'][len(train_data)-1]), str(train_data['question2'][len(train_data)-1]))\n",
    "train_pre.append(tmp)\n",
    "train_df['count_pre'] = train_pre\n",
    "\n",
    "for i in range(0, len(test_data)-1):\n",
    "    tmp = count_pre(str(test_data['question1'][i]), str(test_data['question2'][i]))\n",
    "    test_pre.append(tmp)\n",
    "tmp = count_pre(str(test_data['question1'][len(train_data)-1]), str(test_data['question2'][len(test_data)-1]))\n",
    "test_pre.append(tmp)\n",
    "test_df['count_pre'] = test_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークによる学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# nan削除\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "# テスト用\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_df, train_data, test_size=0.3, random_state=0)\n",
    "\n",
    "# MLPClassifier パラメーター設定\n",
    "clf = MLPClassifier(activation='relu', alpha=0.00011, batch_size=50, beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(50,50,50,50,50,), learning_rate='constant',\n",
    "       learning_rate_init=0.0044, max_iter=1000, random_state=123,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "\n",
    "# テスト用　学習\n",
    "#clf.fit(X_train, y_train['is_duplicate'])\n",
    "#print(clf.score(X_test, y_test['is_duplicate']))\n",
    "#result = clf.predict_proba(X_test)\n",
    "#y_test['result'] = result[:,1]\n",
    "\n",
    "# 学習、モデルセーブ\n",
    "clf.fit(train_df, train_data['is_duplicate'])\n",
    "joblib.dump(clf, \"5feature_model.sav\")\n",
    "\n",
    "# モデルロード\n",
    "#model = joblib.load(\"5feature_model.sav\")\n",
    "\n",
    "# test predict\n",
    "y_test = clf.predict_proba(test_df)\n",
    "\n",
    "# 提出用データ作成\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = test_data['test_id']\n",
    "sub['is_duplicate'] = y_test[:,1]\n",
    "sub.to_csv('submit1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
